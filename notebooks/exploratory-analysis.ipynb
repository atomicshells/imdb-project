{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner.png](banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, min, max, count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIMDbProject\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:503\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    500\u001b[39m     session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSparkSession$\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     ).applyModifiableSettings(session._jsparkSession, \u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1712\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1712\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDbProject\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Load Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMDB_PATH = '../data/raw/imdb'\n",
    "TMDB_PATH = '../data/raw/tmdb'\n",
    "RAW_PARQUET_PATH = '../data/raw_parquet'\n",
    "PROCESSED_PATH = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure output folders exist\n",
    "os.makedirs(RAW_PARQUET_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "imdb_files = {\n",
    "    \"basics\": \"title.basics.tsv\",\n",
    "    \"akas\": \"title.akas.tsv\",\n",
    "    \"crew\": \"title.crew.tsv\",\n",
    "    \"episode\": \"title.episode.tsv\",\n",
    "    \"principals\": \"title.principals.tsv\",\n",
    "    \"ratings\": \"title.ratings.tsv\",\n",
    "    \"names\": \"name.basics.tsv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB data to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb - basics:\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   NULL|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   NULL|             5|     Animation,Short|\n",
      "|tt0000003|    short|        Poor Pierrot|      Pauvre Pierrot|      0|     1892|   NULL|             5|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   NULL|            12|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   NULL|             1|               Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - akas:\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|  titleId|ordering|               title|region|language|      types|   attributes|isOriginalTitle|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|tt0000001|       1|          Carmencita|  NULL|    NULL|   original|         NULL|              1|\n",
      "|tt0000001|       2|          Carmencita|    DE|    NULL|       NULL|literal title|              0|\n",
      "|tt0000001|       3|          Carmencita|    US|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       4|Carmencita - span...|    HU|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       5|          Καρμενσίτα|    GR|    NULL|imdbDisplay|         NULL|              0|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - crew:\n",
      "+---------+---------+---------+\n",
      "|   tconst|directors|  writers|\n",
      "+---------+---------+---------+\n",
      "|tt0000001|nm0005690|     NULL|\n",
      "|tt0000002|nm0721526|     NULL|\n",
      "|tt0000003|nm0721526|nm0721526|\n",
      "|tt0000004|nm0721526|     NULL|\n",
      "|tt0000005|nm0005690|     NULL|\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - episode:\n",
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0031458|  tt32857063|        NULL|         NULL|\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0042889|   tt0989125|        NULL|         NULL|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - principals:\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job|characters|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|tt0000001|       1|nm1588970|           self|                NULL|  [\"Self\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                NULL|      NULL|\n",
      "|tt0000001|       3|nm0005690|       producer|            producer|      NULL|\n",
      "|tt0000001|       4|nm0374658|cinematographer|director of photo...|      NULL|\n",
      "|tt0000002|       1|nm0721526|       director|                NULL|      NULL|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - ratings:\n",
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2136|\n",
      "|tt0000002|          5.5|     289|\n",
      "|tt0000003|          6.4|    2170|\n",
      "|tt0000004|          5.3|     185|\n",
      "|tt0000005|          6.2|    2902|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - names:\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|actor,miscellaneo...|tt0072308,tt00504...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|actress,soundtrac...|tt0037382,tt00752...|\n",
      "|nm0000003|Brigitte Bardot|     1934|     NULL|actress,music_dep...|tt0057345,tt00491...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,writer,musi...|tt0072562,tt00779...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0050986,tt00694...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_dfs = {\n",
    "    key: spark.read.csv(\n",
    "        os.path.join(IMDB_PATH, filename),\n",
    "        sep='\\t',\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        nullValue='\\\\N'\n",
    "    )\n",
    "    for key, filename in imdb_files.items()\n",
    "}\n",
    "\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"IMDb - {name}:\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save unfiltered IMDB files to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, df in imdb_dfs.items():\n",
    "#     parquet_path = os.path.join(RAW_PARQUET_PATH, f'{name}.parquet')\n",
    "#     df.write.mode('overwrite').parquet(parquet_path)\n",
    "#     print(f\"Saved {name} to {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TMDB data to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDb Data Preview:\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|          title|vote_average|vote_count|  status|release_date|   revenue|runtime|adult|       backdrop_path|   budget|            homepage|  imdb_id|original_language| original_title|            overview|          popularity|         poster_path|             tagline|              genres|production_companies|production_countries|    spoken_languages|            keywords|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 27205|      Inception|       8.364|     34495|Released|  2010-07-15| 825532764|    148|False|/8ZTVqvKDQ8emSGUE...|160000000|https://www.warne...|tt1375666|               en|      Inception|\"Cobb, a skilled ...| the implantation...|              83.952|/oYuLEt3zVCKq57qu...|Your mind is the ...|Action, Science F...|Legendary Picture...|United Kingdom, U...|English, French, ...|\n",
      "|157336|   Interstellar|       8.417|     32571|Released|  2014-11-05| 701729206|    169|False|/pbrkL804c8yAv3zB...|165000000|http://www.inters...|tt0816692|               en|   Interstellar|The adventures of...|             140.241|/gEU2QniE6E77NI6l...|Mankind was born ...|Adventure, Drama,...|Legendary Picture...|United Kingdom, U...|             English|rescue, future, s...|\n",
      "|   155|The Dark Knight|       8.512|     30619|Released|  2008-07-16|1004558444|    152|False|/nMKdUUepR0i5zn0y...|185000000|https://www.warne...|tt0468569|               en|The Dark Knight|Batman raises the...|             130.643|/qJ2tW6WMUDux911r...|Welcome to a worl...|Drama, Action, Cr...|DC Comics, Legend...|United Kingdom, U...|   English, Mandarin|joker, sadism, ch...|\n",
      "| 19995|         Avatar|       7.573|     29815|Released|  2009-12-15|2923706026|    162|False|/vL5LR6WdxWPjLPFR...|237000000|https://www.avata...|tt0499549|               en|         Avatar|In the 22nd centu...|              79.932|/kyeqWdyUXW608qlY...|Enter the world o...|Action, Adventure...|Dune Entertainmen...|United States of ...|    English, Spanish|future, society, ...|\n",
      "| 24428|   The Avengers|        7.71|     29166|Released|  2012-04-25|1518815515|    143|False|/9BBTo63ANSmhC4e6...|220000000|https://www.marve...|tt0848228|               en|   The Avengers|When an unexpecte...|              98.082|/RYMX2wcKCBAr24Uy...|Some assembly req...|Science Fiction, ...|      Marvel Studios|United States of ...|English, Hindi, R...|new york city, su...|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmdb_path = os.path.join(TMDB_PATH, 'TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "tmdb_df = spark.read.csv(\n",
    "    tmdb_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "print(\"TMDb Data Preview:\")\n",
    "tmdb_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save TMDB file to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMDB_PARQUET_PATH = '../data/raw_parquet'\n",
    "# os.makedirs(TMDB_PARQUET_PATH, exist_ok=True)\n",
    "\n",
    "# tmdb_df.write.mode('overwrite').parquet(os.path.join(TMDB_PARQUET_PATH, 'TMDB_movie_dataset_v11.parquet'))\n",
    "# print(\"Saved TMDb to parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts per file:\n",
      "basics: 11,485,855 rows\n",
      "akas: 51,496,209 rows\n",
      "crew: 11,485,855 rows\n",
      "episode: 8,832,680 rows\n",
      "principals: 91,154,932 rows\n",
      "ratings: 1,539,107 rows\n",
      "names: 14,217,591 rows\n",
      "+--------+--------+\n",
      "|min_year|max_year|\n",
      "+--------+--------+\n",
      "|    1874|    2031|\n",
      "+--------+--------+\n",
      "\n",
      "+-----------+-------+\n",
      "|      genre|  count|\n",
      "+-----------+-------+\n",
      "|      Drama|3237636|\n",
      "|     Comedy|2236773|\n",
      "|  Talk-Show|1424798|\n",
      "|      Short|1228429|\n",
      "|Documentary|1096234|\n",
      "|       News|1080456|\n",
      "|    Romance|1072271|\n",
      "|     Family| 844672|\n",
      "| Reality-TV| 643344|\n",
      "|  Animation| 572468|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_basics = imdb_dfs['basics']\n",
    "\n",
    "# Total counts for each dataset\n",
    "print(\"Row counts per file:\")\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"{name}: {df.count():,} rows\")\n",
    "\n",
    "# Date range in imdb_basics\n",
    "date_range = imdb_basics.select(min(col(\"startYear\")).alias(\"min_year\"), max(col(\"startYear\")).alias(\"max_year\"))\n",
    "date_range.show()\n",
    "\n",
    "# Top genres\n",
    "genres_exploded = imdb_basics.withColumn(\"genre\", explode(split(col(\"genres\"), \",\")))\n",
    "top_genres = genres_exploded.groupBy(\"genre\").count().orderBy(col(\"count\").desc())\n",
    "top_genres.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Using Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load from saved Parquets (raw)\n",
    "# imdb_dfs = {}\n",
    "# imdb_files = [\"basics\", \"akas\", \"crew\", \"episode\", \"principals\", \"ratings\", \"names\"]\n",
    "\n",
    "# for name in imdb_files:\n",
    "#     path = os.path.join(RAW_PARQUET_PATH, f'{name}.parquet')\n",
    "#     imdb_dfs[name] = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to `movies` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_basics_filtered = imdb_dfs['basics'].filter(col(\"titleType\") == \"movie\")\n",
    "\n",
    "# # Save filtered basics immediately to processed folder\n",
    "# imdb_basics.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'basics.parquet'))\n",
    "\n",
    "imdb_basics_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:503\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:506\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    505\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mError while sending or receiving.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    507\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError while sending\u001b[39m\u001b[33m\"\u001b[39m, e, proto.ERROR_ON_SEND)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Filter akas\u001b[39;00m\n\u001b[32m      2\u001b[39m imdb_akas_filtered = imdb_dfs[\u001b[33m'\u001b[39m\u001b[33makas\u001b[39m\u001b[33m'\u001b[39m] \\\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     .join(\u001b[43mimdb_basics_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtconst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, imdb_dfs[\u001b[33m'\u001b[39m\u001b[33makas\u001b[39m\u001b[33m'\u001b[39m].titleId == imdb_basics_filtered.tconst, \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      4\u001b[39m     .drop(\u001b[33m\"\u001b[39m\u001b[33mtconst\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# imdb_akas.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'akas.parquet'))\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Filter ratings\u001b[39;00m\n\u001b[32m      9\u001b[39m imdb_ratings_filtered = imdb_dfs[\u001b[33m'\u001b[39m\u001b[33mratings\u001b[39m\u001b[33m'\u001b[39m] \\\n\u001b[32m     10\u001b[39m     .join(imdb_basics_filtered.select(\u001b[33m\"\u001b[39m\u001b[33mtconst\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mtconst\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3229\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3185\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3186\u001b[39m \n\u001b[32m   3187\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3227\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3228\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3229\u001b[39m     jdf = \u001b[38;5;28mself\u001b[39m._jdf.select(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2766\u001b[39m, in \u001b[36mDataFrame._jcols\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   2764\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   2765\u001b[39m     cols = cols[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2753\u001b[39m, in \u001b[36mDataFrame._jseq\u001b[39m\u001b[34m(self, cols, converter)\u001b[39m\n\u001b[32m   2747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_jseq\u001b[39m(\n\u001b[32m   2748\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2749\u001b[39m     cols: Sequence,\n\u001b[32m   2750\u001b[39m     converter: Optional[Callable[..., Union[\u001b[33m\"\u001b[39m\u001b[33mPrimitiveType\u001b[39m\u001b[33m\"\u001b[39m, JavaObject]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2751\u001b[39m ) -> JavaObject:\n\u001b[32m   2752\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\column.py:88\u001b[39m, in \u001b[36m_to_seq\u001b[39m\u001b[34m(sc, cols, converter)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03minto JVM Column objects.\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     cols = [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonUtils.toSeq(cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\column.py:63\u001b[39m, in \u001b[36m_to_java_column\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     61\u001b[39m     jcol = col._jc\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     jcol = \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m     66\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\column.py:56\u001b[39m, in \u001b[36m_create_column_from_name\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_column_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mColumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     55\u001b[39m     sc = get_active_spark_context()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctions\u001b[49m.col(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1712\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1712\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1053\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(retry, connection, pne):\n\u001b[32m   1052\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1055\u001b[39m     logging.exception(\n\u001b[32m   1056\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# Filter akas\n",
    "imdb_akas_filtered = imdb_dfs['akas'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), imdb_dfs['akas'].titleId == imdb_basics_filtered.tconst, \"inner\") \\\n",
    "    .drop(\"tconst\")\n",
    "\n",
    "# imdb_akas.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'akas.parquet'))\n",
    "\n",
    "# Filter ratings\n",
    "imdb_ratings_filtered = imdb_dfs['ratings'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_ratings.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'ratings.parquet'))\n",
    "\n",
    "# Filter principals\n",
    "imdb_principals_filtered = imdb_dfs['principals'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_principals.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'principals.parquet'))\n",
    "\n",
    "# Filter crew\n",
    "imdb_crew_filtered = imdb_dfs['crew'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_crew.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'crew.parquet'))\n",
    "\n",
    "# For now, leaving `names` alone since it's a people table not directly tied to titleType.\n",
    "imdb_names = imdb_dfs['names']\n",
    "# imdb_dfs['names'].write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'names.parquet'))\n",
    "\n",
    "# Episodes is usually for TV shows, so we can skip it, or handle if needed later.\n",
    "imdb_episodes = imdb_dfs['episode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o179.count.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3684/82042809.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3615)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3607/376411126.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2021/1075174821.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1673/717076099.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1684/1192951288.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1674/262707506.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\tSuppressed: org.apache.spark.SparkException: Job 53 cancelled because SparkContext was shut down\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4766/903179147.apply(Unknown Source)\r\n\t\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\r\n\t\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4765/1916375042.apply$mcV$sp(Unknown Source)\r\n\t\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\r\n\t\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\r\n\t\tat org.apache.spark.SparkContext$$Lambda$4762/500915728.apply$mcV$sp(Unknown Source)\r\n\t\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\t\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\r\n\t\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\r\n\t\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\r\n\t\tat org.apache.spark.SparkContext$$Lambda$734/1709177787.apply$mcV$sp(Unknown Source)\r\n\t\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4732/1041841685.apply$mcV$sp(Unknown Source)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\t\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4731/975613321.apply$mcV$sp(Unknown Source)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\t\tat scala.util.Try$.apply(Try.scala:213)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\t\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\t\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Job 54 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4766/903179147.apply(Unknown Source)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4765/1916375042.apply$mcV$sp(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext$$Lambda$4762/500915728.apply$mcV$sp(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\r\n\tat org.apache.spark.SparkContext$$Lambda$734/1709177787.apply$mcV$sp(Unknown Source)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4732/1041841685.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4731/975613321.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m imdb_basics_filtered.count()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mimdb_akas_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m imdb_ratings_filtered.count()\n\u001b[32m      4\u001b[39m imdb_principals_filtered.count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1240\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   1220\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o179.count.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3684/82042809.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3615)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3607/376411126.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2021/1075174821.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1673/717076099.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1684/1192951288.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1674/262707506.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\tSuppressed: org.apache.spark.SparkException: Job 53 cancelled because SparkContext was shut down\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4766/903179147.apply(Unknown Source)\r\n\t\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\r\n\t\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4765/1916375042.apply$mcV$sp(Unknown Source)\r\n\t\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\r\n\t\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\r\n\t\tat org.apache.spark.SparkContext$$Lambda$4762/500915728.apply$mcV$sp(Unknown Source)\r\n\t\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\t\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\r\n\t\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\r\n\t\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\r\n\t\tat org.apache.spark.SparkContext$$Lambda$734/1709177787.apply$mcV$sp(Unknown Source)\r\n\t\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4732/1041841685.apply$mcV$sp(Unknown Source)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\t\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4731/975613321.apply$mcV$sp(Unknown Source)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\t\tat scala.util.Try$.apply(Try.scala:213)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\t\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\t\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\t\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Job 54 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4766/903179147.apply(Unknown Source)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4765/1916375042.apply$mcV$sp(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext$$Lambda$4762/500915728.apply$mcV$sp(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\r\n\tat org.apache.spark.SparkContext$$Lambda$734/1709177787.apply$mcV$sp(Unknown Source)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4732/1041841685.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$Lambda$4731/975613321.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "imdb_basics_filtered.count()\n",
    "imdb_akas_filtered.count()\n",
    "imdb_ratings_filtered.count()\n",
    "imdb_principals_filtered.count()\n",
    "imdb_crew_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_movies.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb Basics Schema:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'imdb_basics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIMDb Basics Schema:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mimdb_basics\u001b[49m.printSchema()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTMDb Schema:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m tmdb_movies.printSchema()\n",
      "\u001b[31mNameError\u001b[39m: name 'imdb_basics' is not defined"
     ]
    }
   ],
   "source": [
    "imdb_basics_filtered.printSchema()\n",
    "imdb_akas_filtered.printSchema()\n",
    "imdb_ratings_filtered.printSchema()\n",
    "imdb_principals_filtered.printSchema()\n",
    "imdb_crew_filtered.printSchema()\n",
    "imdb_names.printSchema()\n",
    "imdb_episode.printSchema()\n",
    "\n",
    "tmdb_movies.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Highest Rated Movies in 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balancing Ratings and Number of Votes:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Most Popular Actors/Actresses in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. User-Movie Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Metric of a 'Hit Movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Predictive Modelling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
