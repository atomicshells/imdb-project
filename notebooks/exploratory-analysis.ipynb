{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner.png](banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, min, max, count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HADOOP_HOME\"] = \"D:\\\\hadoop\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 21:48:29 WARN Utils: Your hostname, LAPTOP-AE18MK00 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/08 21:48:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/08 21:48:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDbProject\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "# Confirm Spark is running\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Load Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMDB_PATH = '../data/raw/imdb'\n",
    "TMDB_PATH = '../data/raw/tmdb'\n",
    "RAW_PARQUET_PATH = '../data/raw_parquet'\n",
    "PROCESSED_PATH = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load just ratings file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ratings = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m.csv(\n\u001b[32m      5\u001b[39m     os.path.join(IMDB_PATH, \u001b[33m'\u001b[39m\u001b[33mtitle.ratings.tsv\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m      6\u001b[39m     sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     header=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     nullValue=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Quick check the schema (optional)\u001b[39;00m\n\u001b[32m     13\u001b[39m ratings.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/pyspark/sql/session.py:1706\u001b[39m, in \u001b[36mSparkSession.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1669\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   1670\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrameReader:\n\u001b[32m   1671\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1672\u001b[39m \u001b[33;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[32m   1673\u001b[39m \u001b[33;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1704\u001b[39m \u001b[33;03m    +---+------------+\u001b[39;00m\n\u001b[32m   1705\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:70\u001b[39m, in \u001b[36mDataFrameReader.__init__\u001b[39m\u001b[34m(self, spark)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[33m\"\u001b[39m\u001b[33mSparkSession\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28mself\u001b[39m._jreader = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m._spark = spark\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/imdb-project-venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Load just ratings file\n",
    "ratings = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, 'title.ratings.tsv'),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "# Quick check the schema (optional)\n",
    "ratings.printSchema()\n",
    "\n",
    "# Quick check the data (optional)\n",
    "ratings.show(5)\n",
    "\n",
    "# Test writing ratings to Parquet (this is the test)\n",
    "ratings.write.mode('overwrite').parquet(os.path.join(PROCESSED_PATH, 'ratings.parquet'))\n",
    "\n",
    "print(\"✅ Ratings saved to Parquet successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure output folders exist\n",
    "os.makedirs(RAW_PARQUET_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "imdb_files = {\n",
    "    \"basics\": \"title.basics.tsv\",\n",
    "    \"akas\": \"title.akas.tsv\",\n",
    "    \"crew\": \"title.crew.tsv\",\n",
    "    \"episode\": \"title.episode.tsv\",\n",
    "    \"principals\": \"title.principals.tsv\",\n",
    "    \"ratings\": \"title.ratings.tsv\",\n",
    "    \"names\": \"name.basics.tsv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB data to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb - basics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   NULL|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   NULL|             5|     Animation,Short|\n",
      "|tt0000003|    short|        Poor Pierrot|      Pauvre Pierrot|      0|     1892|   NULL|             5|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   NULL|            12|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   NULL|             1|               Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - akas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|  titleId|ordering|               title|region|language|      types|   attributes|isOriginalTitle|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|tt0000001|       1|          Carmencita|  NULL|    NULL|   original|         NULL|              1|\n",
      "|tt0000001|       2|          Carmencita|    DE|    NULL|       NULL|literal title|              0|\n",
      "|tt0000001|       3|          Carmencita|    US|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       4|Carmencita - span...|    HU|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       5|          Καρμενσίτα|    GR|    NULL|imdbDisplay|         NULL|              0|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - crew:\n",
      "+---------+---------+---------+\n",
      "|   tconst|directors|  writers|\n",
      "+---------+---------+---------+\n",
      "|tt0000001|nm0005690|     NULL|\n",
      "|tt0000002|nm0721526|     NULL|\n",
      "|tt0000003|nm0721526|nm0721526|\n",
      "|tt0000004|nm0721526|     NULL|\n",
      "|tt0000005|nm0005690|     NULL|\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - episode:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0031458|  tt32857063|        NULL|         NULL|\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0042889|   tt0989125|        NULL|         NULL|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - principals:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job|characters|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|tt0000001|       1|nm1588970|           self|                NULL|  [\"Self\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                NULL|      NULL|\n",
      "|tt0000001|       3|nm0005690|       producer|            producer|      NULL|\n",
      "|tt0000001|       4|nm0374658|cinematographer|director of photo...|      NULL|\n",
      "|tt0000002|       1|nm0721526|       director|                NULL|      NULL|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - ratings:\n",
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2136|\n",
      "|tt0000002|          5.5|     289|\n",
      "|tt0000003|          6.4|    2170|\n",
      "|tt0000004|          5.3|     185|\n",
      "|tt0000005|          6.2|    2902|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - names:\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|actor,miscellaneo...|tt0072308,tt00504...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|actress,soundtrac...|tt0037382,tt00752...|\n",
      "|nm0000003|Brigitte Bardot|     1934|     NULL|actress,music_dep...|tt0057345,tt00491...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,writer,musi...|tt0072562,tt00779...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0050986,tt00694...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_dfs = {\n",
    "    key: spark.read.csv(\n",
    "        os.path.join(IMDB_PATH, filename),\n",
    "        sep='\\t',\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        nullValue='\\\\N'\n",
    "    )\n",
    "    for key, filename in imdb_files.items()\n",
    "}\n",
    "\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"IMDb - {name}:\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save unfiltered IMDB files to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving 'ratings' to ../data/raw_parquet/ratings.parquet ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved 'ratings' to ../data/raw_parquet/ratings.parquet!\n"
     ]
    }
   ],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'ratings.parquet')\n",
    "print(f\"💾 Saving 'ratings' to {parquet_path} ...\")\n",
    "imdb_dfs['ratings'].write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'ratings' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving 'names' to ../data/raw_parquet/names.parquet ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==========================================================(8 + 0) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved 'names' to ../data/raw_parquet/names.parquet!\n"
     ]
    }
   ],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'names.parquet')\n",
    "print(f\"💾 Saving 'names' to {parquet_path} ...\")\n",
    "imdb_dfs['names'].write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'names' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading IMDb dataset: basics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 'basics'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"📂 Loading IMDb dataset: basics\")\n",
    "imdb_df_basics = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, \"title.basics.tsv\"),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "print(f\"✅ Loaded 'basics'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving 'basics' to ../data/raw_parquet/basics.parquet ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 21:32:09 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false) 8]\n",
      "25/03/08 21:32:09 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1219)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:48)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n"
     ]
    }
   ],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'basics.parquet')\n",
    "print(f\"💾 Saving 'basics' to {parquet_path} ...\")\n",
    "imdb_df_basics.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'basics' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading IMDb dataset: akas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (18 + 2) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 'akas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"📂 Loading IMDb dataset: akas\")\n",
    "imdb_df_akas = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, \"title.akas.tsv\"),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "print(f\"✅ Loaded 'akas'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'akas.parquet')\n",
    "print(f\"💾 Saving 'akas' to {parquet_path} ...\")\n",
    "imdb_df_akas.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'akas' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📂 Loading IMDb dataset: crew\")\n",
    "imdb_df_crew = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, \"title.crew.tsv\"),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "print(f\"✅ Loaded 'crew'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'crew.parquet')\n",
    "print(f\"💾 Saving 'crew' to {parquet_path} ...\")\n",
    "imdb_df_crew.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'crew' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📂 Loading IMDb dataset: principals\")\n",
    "imdb_df_principals = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, \"title.principals.tsv\"),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "print(f\"✅ Loaded 'principals'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'principals.parquet')\n",
    "print(f\"💾 Saving 'principals' to {parquet_path} ...\")\n",
    "imdb_df_principals.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'principals' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📂 Loading IMDb dataset: episode\")\n",
    "imdb_df_episode = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, \"title.episode.tsv\"),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "print(f\"✅ Loaded 'episode'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = os.path.join(RAW_PARQUET_PATH, 'episode.parquet')\n",
    "print(f\"💾 Saving 'episode' to {parquet_path} ...\")\n",
    "imdb_df_episode.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✅ Successfully saved 'episode' to {parquet_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TMDB data to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_path = os.path.join(TMDB_PATH, 'TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "tmdb_df = spark.read.csv(\n",
    "    tmdb_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "print(\"TMDb Data Preview:\")\n",
    "tmdb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDb Data Preview:\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|          title|vote_average|vote_count|  status|release_date|   revenue|runtime|adult|       backdrop_path|   budget|            homepage|  imdb_id|original_language| original_title|            overview|          popularity|         poster_path|             tagline|              genres|production_companies|production_countries|    spoken_languages|            keywords|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 27205|      Inception|       8.364|     34495|Released|  2010-07-15| 825532764|    148|False|/8ZTVqvKDQ8emSGUE...|160000000|https://www.warne...|tt1375666|               en|      Inception|\"Cobb, a skilled ...| the implantation...|              83.952|/oYuLEt3zVCKq57qu...|Your mind is the ...|Action, Science F...|Legendary Picture...|United Kingdom, U...|English, French, ...|\n",
      "|157336|   Interstellar|       8.417|     32571|Released|  2014-11-05| 701729206|    169|False|/pbrkL804c8yAv3zB...|165000000|http://www.inters...|tt0816692|               en|   Interstellar|The adventures of...|             140.241|/gEU2QniE6E77NI6l...|Mankind was born ...|Adventure, Drama,...|Legendary Picture...|United Kingdom, U...|             English|rescue, future, s...|\n",
      "|   155|The Dark Knight|       8.512|     30619|Released|  2008-07-16|1004558444|    152|False|/nMKdUUepR0i5zn0y...|185000000|https://www.warne...|tt0468569|               en|The Dark Knight|Batman raises the...|             130.643|/qJ2tW6WMUDux911r...|Welcome to a worl...|Drama, Action, Cr...|DC Comics, Legend...|United Kingdom, U...|   English, Mandarin|joker, sadism, ch...|\n",
      "| 19995|         Avatar|       7.573|     29815|Released|  2009-12-15|2923706026|    162|False|/vL5LR6WdxWPjLPFR...|237000000|https://www.avata...|tt0499549|               en|         Avatar|In the 22nd centu...|              79.932|/kyeqWdyUXW608qlY...|Enter the world o...|Action, Adventure...|Dune Entertainmen...|United States of ...|    English, Spanish|future, society, ...|\n",
      "| 24428|   The Avengers|        7.71|     29166|Released|  2012-04-25|1518815515|    143|False|/9BBTo63ANSmhC4e6...|220000000|https://www.marve...|tt0848228|               en|   The Avengers|When an unexpecte...|              98.082|/RYMX2wcKCBAr24Uy...|Some assembly req...|Science Fiction, ...|      Marvel Studios|United States of ...|English, Hindi, R...|new york city, su...|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmdb_path = os.path.join(TMDB_PATH, 'TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "tmdb_df = spark.read.csv(\n",
    "    tmdb_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "print(\"TMDb Data Preview:\")\n",
    "tmdb_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save TMDB file to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_PARQUET_PATH = '../data/raw_parquet'\n",
    "os.makedirs(TMDB_PARQUET_PATH, exist_ok=True)\n",
    "\n",
    "tmdb_df.write.mode('overwrite').parquet(os.path.join(TMDB_PARQUET_PATH, 'TMDB_movie_dataset_v11.parquet'))\n",
    "print(\"Saved TMDb to parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts per file:\n",
      "basics: 11,485,855 rows\n",
      "akas: 51,496,209 rows\n",
      "crew: 11,485,855 rows\n",
      "episode: 8,832,680 rows\n",
      "principals: 91,154,932 rows\n",
      "ratings: 1,539,107 rows\n",
      "names: 14,217,591 rows\n",
      "+--------+--------+\n",
      "|min_year|max_year|\n",
      "+--------+--------+\n",
      "|    1874|    2031|\n",
      "+--------+--------+\n",
      "\n",
      "+-----------+-------+\n",
      "|      genre|  count|\n",
      "+-----------+-------+\n",
      "|      Drama|3237636|\n",
      "|     Comedy|2236773|\n",
      "|  Talk-Show|1424798|\n",
      "|      Short|1228429|\n",
      "|Documentary|1096234|\n",
      "|       News|1080456|\n",
      "|    Romance|1072271|\n",
      "|     Family| 844672|\n",
      "| Reality-TV| 643344|\n",
      "|  Animation| 572468|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_basics = imdb_dfs['basics']\n",
    "\n",
    "# Total counts for each dataset\n",
    "print(\"Row counts per file:\")\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"{name}: {df.count():,} rows\")\n",
    "\n",
    "# Date range in imdb_basics\n",
    "date_range = imdb_basics.select(min(col(\"startYear\")).alias(\"min_year\"), max(col(\"startYear\")).alias(\"max_year\"))\n",
    "date_range.show()\n",
    "\n",
    "# Top genres\n",
    "genres_exploded = imdb_basics.withColumn(\"genre\", explode(split(col(\"genres\"), \",\")))\n",
    "top_genres = genres_exploded.groupBy(\"genre\").count().orderBy(col(\"count\").desc())\n",
    "top_genres.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Using Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load from saved Parquets (raw)\n",
    "# imdb_dfs = {}\n",
    "# imdb_files = [\"basics\", \"akas\", \"crew\", \"episode\", \"principals\", \"ratings\", \"names\"]\n",
    "\n",
    "# for name in imdb_files:\n",
    "#     path = os.path.join(RAW_PARQUET_PATH, f'{name}.parquet')\n",
    "#     imdb_dfs[name] = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to `movies` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_basics_filtered = imdb_dfs['basics'].filter(col(\"titleType\") == \"movie\")\n",
    "\n",
    "# # Save filtered basics immediately to processed folder\n",
    "# imdb_basics.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'basics.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter akas\n",
    "imdb_akas_filtered = imdb_dfs['akas'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), imdb_dfs['akas'].titleId == imdb_basics_filtered.tconst, \"inner\") \\\n",
    "    .drop(\"tconst\")\n",
    "\n",
    "# imdb_akas.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'akas.parquet'))\n",
    "\n",
    "# Filter ratings\n",
    "imdb_ratings_filtered = imdb_dfs['ratings'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_ratings.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'ratings.parquet'))\n",
    "\n",
    "# Filter principals\n",
    "imdb_principals_filtered = imdb_dfs['principals'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_principals.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'principals.parquet'))\n",
    "\n",
    "# Filter crew\n",
    "imdb_crew_filtered = imdb_dfs['crew'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_crew.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'crew.parquet'))\n",
    "\n",
    "# For now, leaving `names` alone since it's a people table not directly tied to titleType.\n",
    "imdb_names = imdb_dfs['names']\n",
    "# imdb_dfs['names'].write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'names.parquet'))\n",
    "\n",
    "# Episodes is usually for TV shows, so we can skip it, or handle if needed later.\n",
    "imdb_episodes = imdb_dfs['episode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_basics_filtered.count()\n",
    "imdb_akas_filtered.count()\n",
    "imdb_ratings_filtered.count()\n",
    "imdb_principals_filtered.count()\n",
    "imdb_crew_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_movies.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schemas for IMDb files\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"Schema for IMDb file: {name}\")\n",
    "    df.printSchema()\n",
    "    print(\"=\"*40)  # Just for cleaner separation\n",
    "\n",
    "# Print schema for TMDb file\n",
    "print(\"Schema for TMDb file:\")\n",
    "tmdb_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Highest Rated Movies in 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balancing Ratings and Number of Votes:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter 2023 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Most Popular Actors/Actresses in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. User-Movie Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Metric of a 'Hit Movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Predictive Modelling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
