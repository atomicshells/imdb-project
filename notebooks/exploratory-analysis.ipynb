{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner.png](banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, min, max, count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HADOOP_HOME\"] = \"D:\\\\hadoop\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDbProject\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "# Confirm Spark is running\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Load Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMDB_PATH = '../data/raw/imdb'\n",
    "TMDB_PATH = '../data/raw/tmdb'\n",
    "RAW_PARQUET_PATH = '../data/raw_parquet'\n",
    "PROCESSED_PATH = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: double (nullable = true)\n",
      " |-- numVotes: integer (nullable = true)\n",
      "\n",
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2136|\n",
      "|tt0000002|          5.5|     289|\n",
      "|tt0000003|          6.4|    2170|\n",
      "|tt0000004|          5.3|     185|\n",
      "|tt0000005|          6.2|    2902|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.parquet.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3458/1892303904.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1$$Lambda$3099/449493856.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1698/742613637.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1688/126248078.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$1287/273212321.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m ratings.show(\u001b[32m5\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Test writing ratings to Parquet (this is the test)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mratings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROCESSED_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mratings.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Ratings saved to Parquet successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   1719\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   1720\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m1721\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVC\\Work\\GCash DSAI Project\\imdb-project\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o39.parquet.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3458/1892303904.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1$$Lambda$3099/449493856.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1698/742613637.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1688/126248078.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$1287/273212321.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Load just ratings file\n",
    "ratings = spark.read.csv(\n",
    "    os.path.join(IMDB_PATH, 'title.ratings.tsv'),\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "# Quick check the schema (optional)\n",
    "ratings.printSchema()\n",
    "\n",
    "# Quick check the data (optional)\n",
    "ratings.show(5)\n",
    "\n",
    "# Test writing ratings to Parquet (this is the test)\n",
    "ratings.write.mode('overwrite').parquet(os.path.join(PROCESSED_PATH, 'ratings.parquet'))\n",
    "\n",
    "print(\"✅ Ratings saved to Parquet successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure output folders exist\n",
    "os.makedirs(RAW_PARQUET_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "imdb_files = {\n",
    "    \"basics\": \"title.basics.tsv\",\n",
    "    \"akas\": \"title.akas.tsv\",\n",
    "    \"crew\": \"title.crew.tsv\",\n",
    "    \"episode\": \"title.episode.tsv\",\n",
    "    \"principals\": \"title.principals.tsv\",\n",
    "    \"ratings\": \"title.ratings.tsv\",\n",
    "    \"names\": \"name.basics.tsv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB data to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb - basics:\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   NULL|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   NULL|             5|     Animation,Short|\n",
      "|tt0000003|    short|        Poor Pierrot|      Pauvre Pierrot|      0|     1892|   NULL|             5|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   NULL|            12|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   NULL|             1|               Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - akas:\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|  titleId|ordering|               title|region|language|      types|   attributes|isOriginalTitle|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|tt0000001|       1|          Carmencita|  NULL|    NULL|   original|         NULL|              1|\n",
      "|tt0000001|       2|          Carmencita|    DE|    NULL|       NULL|literal title|              0|\n",
      "|tt0000001|       3|          Carmencita|    US|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       4|Carmencita - span...|    HU|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       5|          Καρμενσίτα|    GR|    NULL|imdbDisplay|         NULL|              0|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - crew:\n",
      "+---------+---------+---------+\n",
      "|   tconst|directors|  writers|\n",
      "+---------+---------+---------+\n",
      "|tt0000001|nm0005690|     NULL|\n",
      "|tt0000002|nm0721526|     NULL|\n",
      "|tt0000003|nm0721526|nm0721526|\n",
      "|tt0000004|nm0721526|     NULL|\n",
      "|tt0000005|nm0005690|     NULL|\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - episode:\n",
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0031458|  tt32857063|        NULL|         NULL|\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0042889|   tt0989125|        NULL|         NULL|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - principals:\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job|characters|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|tt0000001|       1|nm1588970|           self|                NULL|  [\"Self\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                NULL|      NULL|\n",
      "|tt0000001|       3|nm0005690|       producer|            producer|      NULL|\n",
      "|tt0000001|       4|nm0374658|cinematographer|director of photo...|      NULL|\n",
      "|tt0000002|       1|nm0721526|       director|                NULL|      NULL|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - ratings:\n",
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2136|\n",
      "|tt0000002|          5.5|     289|\n",
      "|tt0000003|          6.4|    2170|\n",
      "|tt0000004|          5.3|     185|\n",
      "|tt0000005|          6.2|    2902|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - names:\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|actor,miscellaneo...|tt0072308,tt00504...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|actress,soundtrac...|tt0037382,tt00752...|\n",
      "|nm0000003|Brigitte Bardot|     1934|     NULL|actress,music_dep...|tt0057345,tt00491...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,writer,musi...|tt0072562,tt00779...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0050986,tt00694...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_dfs = {\n",
    "    key: spark.read.csv(\n",
    "        os.path.join(IMDB_PATH, filename),\n",
    "        sep='\\t',\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        nullValue='\\\\N'\n",
    "    )\n",
    "    for key, filename in imdb_files.items()\n",
    "}\n",
    "\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"IMDb - {name}:\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb - basics:\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   NULL|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   NULL|             5|     Animation,Short|\n",
      "|tt0000003|    short|        Poor Pierrot|      Pauvre Pierrot|      0|     1892|   NULL|             5|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   NULL|            12|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   NULL|             1|               Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - akas:\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|  titleId|ordering|               title|region|language|      types|   attributes|isOriginalTitle|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|tt0000001|       1|          Carmencita|  NULL|    NULL|   original|         NULL|              1|\n",
      "|tt0000001|       2|          Carmencita|    DE|    NULL|       NULL|literal title|              0|\n",
      "|tt0000001|       3|          Carmencita|    US|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       4|Carmencita - span...|    HU|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       5|          Καρμενσίτα|    GR|    NULL|imdbDisplay|         NULL|              0|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - crew:\n",
      "+---------+---------+---------+\n",
      "|   tconst|directors|  writers|\n",
      "+---------+---------+---------+\n",
      "|tt0000001|nm0005690|     NULL|\n",
      "|tt0000002|nm0721526|     NULL|\n",
      "|tt0000003|nm0721526|nm0721526|\n",
      "|tt0000004|nm0721526|     NULL|\n",
      "|tt0000005|nm0005690|     NULL|\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - episode:\n",
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0031458|  tt32857063|        NULL|         NULL|\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0042889|   tt0989125|        NULL|         NULL|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - principals:\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job|characters|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|tt0000001|       1|nm1588970|           self|                NULL|  [\"Self\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                NULL|      NULL|\n",
      "|tt0000001|       3|nm0005690|       producer|            producer|      NULL|\n",
      "|tt0000001|       4|nm0374658|cinematographer|director of photo...|      NULL|\n",
      "|tt0000002|       1|nm0721526|       director|                NULL|      NULL|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - ratings:\n",
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2136|\n",
      "|tt0000002|          5.5|     289|\n",
      "|tt0000003|          6.4|    2170|\n",
      "|tt0000004|          5.3|     185|\n",
      "|tt0000005|          6.2|    2902|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "IMDb - names:\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|actor,miscellaneo...|tt0072308,tt00504...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|actress,soundtrac...|tt0037382,tt00752...|\n",
      "|nm0000003|Brigitte Bardot|     1934|     NULL|actress,music_dep...|tt0057345,tt00491...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,writer,musi...|tt0072562,tt00779...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0050986,tt00694...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_dfs = {\n",
    "    key: spark.read.csv(\n",
    "        os.path.join(IMDB_PATH, filename),\n",
    "        sep='\\t',\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        nullValue='\\\\N'\n",
    "    )\n",
    "    for key, filename in imdb_files.items()\n",
    "}\n",
    "\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"IMDb - {name}:\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save unfiltered IMDB files to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, df in imdb_dfs.items():\n",
    "#     parquet_path = os.path.join(RAW_PARQUET_PATH, f'{name}.parquet')\n",
    "#     df.write.mode('overwrite').parquet(parquet_path)\n",
    "#     print(f\"Saved {name} to {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TMDB data to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDb Data Preview:\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|          title|vote_average|vote_count|  status|release_date|   revenue|runtime|adult|       backdrop_path|   budget|            homepage|  imdb_id|original_language| original_title|            overview|          popularity|         poster_path|             tagline|              genres|production_companies|production_countries|    spoken_languages|            keywords|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 27205|      Inception|       8.364|     34495|Released|  2010-07-15| 825532764|    148|False|/8ZTVqvKDQ8emSGUE...|160000000|https://www.warne...|tt1375666|               en|      Inception|\"Cobb, a skilled ...| the implantation...|              83.952|/oYuLEt3zVCKq57qu...|Your mind is the ...|Action, Science F...|Legendary Picture...|United Kingdom, U...|English, French, ...|\n",
      "|157336|   Interstellar|       8.417|     32571|Released|  2014-11-05| 701729206|    169|False|/pbrkL804c8yAv3zB...|165000000|http://www.inters...|tt0816692|               en|   Interstellar|The adventures of...|             140.241|/gEU2QniE6E77NI6l...|Mankind was born ...|Adventure, Drama,...|Legendary Picture...|United Kingdom, U...|             English|rescue, future, s...|\n",
      "|   155|The Dark Knight|       8.512|     30619|Released|  2008-07-16|1004558444|    152|False|/nMKdUUepR0i5zn0y...|185000000|https://www.warne...|tt0468569|               en|The Dark Knight|Batman raises the...|             130.643|/qJ2tW6WMUDux911r...|Welcome to a worl...|Drama, Action, Cr...|DC Comics, Legend...|United Kingdom, U...|   English, Mandarin|joker, sadism, ch...|\n",
      "| 19995|         Avatar|       7.573|     29815|Released|  2009-12-15|2923706026|    162|False|/vL5LR6WdxWPjLPFR...|237000000|https://www.avata...|tt0499549|               en|         Avatar|In the 22nd centu...|              79.932|/kyeqWdyUXW608qlY...|Enter the world o...|Action, Adventure...|Dune Entertainmen...|United States of ...|    English, Spanish|future, society, ...|\n",
      "| 24428|   The Avengers|        7.71|     29166|Released|  2012-04-25|1518815515|    143|False|/9BBTo63ANSmhC4e6...|220000000|https://www.marve...|tt0848228|               en|   The Avengers|When an unexpecte...|              98.082|/RYMX2wcKCBAr24Uy...|Some assembly req...|Science Fiction, ...|      Marvel Studios|United States of ...|English, Hindi, R...|new york city, su...|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmdb_path = os.path.join(TMDB_PATH, 'TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "tmdb_df = spark.read.csv(\n",
    "    tmdb_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "print(\"TMDb Data Preview:\")\n",
    "tmdb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDb Data Preview:\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|          title|vote_average|vote_count|  status|release_date|   revenue|runtime|adult|       backdrop_path|   budget|            homepage|  imdb_id|original_language| original_title|            overview|          popularity|         poster_path|             tagline|              genres|production_companies|production_countries|    spoken_languages|            keywords|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 27205|      Inception|       8.364|     34495|Released|  2010-07-15| 825532764|    148|False|/8ZTVqvKDQ8emSGUE...|160000000|https://www.warne...|tt1375666|               en|      Inception|\"Cobb, a skilled ...| the implantation...|              83.952|/oYuLEt3zVCKq57qu...|Your mind is the ...|Action, Science F...|Legendary Picture...|United Kingdom, U...|English, French, ...|\n",
      "|157336|   Interstellar|       8.417|     32571|Released|  2014-11-05| 701729206|    169|False|/pbrkL804c8yAv3zB...|165000000|http://www.inters...|tt0816692|               en|   Interstellar|The adventures of...|             140.241|/gEU2QniE6E77NI6l...|Mankind was born ...|Adventure, Drama,...|Legendary Picture...|United Kingdom, U...|             English|rescue, future, s...|\n",
      "|   155|The Dark Knight|       8.512|     30619|Released|  2008-07-16|1004558444|    152|False|/nMKdUUepR0i5zn0y...|185000000|https://www.warne...|tt0468569|               en|The Dark Knight|Batman raises the...|             130.643|/qJ2tW6WMUDux911r...|Welcome to a worl...|Drama, Action, Cr...|DC Comics, Legend...|United Kingdom, U...|   English, Mandarin|joker, sadism, ch...|\n",
      "| 19995|         Avatar|       7.573|     29815|Released|  2009-12-15|2923706026|    162|False|/vL5LR6WdxWPjLPFR...|237000000|https://www.avata...|tt0499549|               en|         Avatar|In the 22nd centu...|              79.932|/kyeqWdyUXW608qlY...|Enter the world o...|Action, Adventure...|Dune Entertainmen...|United States of ...|    English, Spanish|future, society, ...|\n",
      "| 24428|   The Avengers|        7.71|     29166|Released|  2012-04-25|1518815515|    143|False|/9BBTo63ANSmhC4e6...|220000000|https://www.marve...|tt0848228|               en|   The Avengers|When an unexpecte...|              98.082|/RYMX2wcKCBAr24Uy...|Some assembly req...|Science Fiction, ...|      Marvel Studios|United States of ...|English, Hindi, R...|new york city, su...|\n",
      "+------+---------------+------------+----------+--------+------------+----------+-------+-----+--------------------+---------+--------------------+---------+-----------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmdb_path = os.path.join(TMDB_PATH, 'TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "tmdb_df = spark.read.csv(\n",
    "    tmdb_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue='\\\\N'\n",
    ")\n",
    "\n",
    "print(\"TMDb Data Preview:\")\n",
    "tmdb_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save TMDB file to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMDB_PARQUET_PATH = '../data/raw_parquet'\n",
    "# os.makedirs(TMDB_PARQUET_PATH, exist_ok=True)\n",
    "\n",
    "# tmdb_df.write.mode('overwrite').parquet(os.path.join(TMDB_PARQUET_PATH, 'TMDB_movie_dataset_v11.parquet'))\n",
    "# print(\"Saved TMDb to parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts per file:\n",
      "basics: 11,485,855 rows\n",
      "akas: 51,496,209 rows\n",
      "crew: 11,485,855 rows\n",
      "episode: 8,832,680 rows\n",
      "principals: 91,154,932 rows\n",
      "ratings: 1,539,107 rows\n",
      "names: 14,217,591 rows\n",
      "+--------+--------+\n",
      "|min_year|max_year|\n",
      "+--------+--------+\n",
      "|    1874|    2031|\n",
      "+--------+--------+\n",
      "\n",
      "+-----------+-------+\n",
      "|      genre|  count|\n",
      "+-----------+-------+\n",
      "|      Drama|3237636|\n",
      "|     Comedy|2236773|\n",
      "|  Talk-Show|1424798|\n",
      "|      Short|1228429|\n",
      "|Documentary|1096234|\n",
      "|       News|1080456|\n",
      "|    Romance|1072271|\n",
      "|     Family| 844672|\n",
      "| Reality-TV| 643344|\n",
      "|  Animation| 572468|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_basics = imdb_dfs['basics']\n",
    "\n",
    "# Total counts for each dataset\n",
    "print(\"Row counts per file:\")\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"{name}: {df.count():,} rows\")\n",
    "\n",
    "# Date range in imdb_basics\n",
    "date_range = imdb_basics.select(min(col(\"startYear\")).alias(\"min_year\"), max(col(\"startYear\")).alias(\"max_year\"))\n",
    "date_range.show()\n",
    "\n",
    "# Top genres\n",
    "genres_exploded = imdb_basics.withColumn(\"genre\", explode(split(col(\"genres\"), \",\")))\n",
    "top_genres = genres_exploded.groupBy(\"genre\").count().orderBy(col(\"count\").desc())\n",
    "top_genres.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Using Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load from saved Parquets (raw)\n",
    "# imdb_dfs = {}\n",
    "# imdb_files = [\"basics\", \"akas\", \"crew\", \"episode\", \"principals\", \"ratings\", \"names\"]\n",
    "\n",
    "# for name in imdb_files:\n",
    "#     path = os.path.join(RAW_PARQUET_PATH, f'{name}.parquet')\n",
    "#     imdb_dfs[name] = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to `movies` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_basics_filtered = imdb_dfs['basics'].filter(col(\"titleType\") == \"movie\")\n",
    "\n",
    "# # Save filtered basics immediately to processed folder\n",
    "# imdb_basics.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'basics.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter akas\n",
    "imdb_akas_filtered = imdb_dfs['akas'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), imdb_dfs['akas'].titleId == imdb_basics_filtered.tconst, \"inner\") \\\n",
    "    .drop(\"tconst\")\n",
    "\n",
    "# imdb_akas.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'akas.parquet'))\n",
    "\n",
    "# Filter ratings\n",
    "imdb_ratings_filtered = imdb_dfs['ratings'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_ratings.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'ratings.parquet'))\n",
    "\n",
    "# Filter principals\n",
    "imdb_principals_filtered = imdb_dfs['principals'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_principals.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'principals.parquet'))\n",
    "\n",
    "# Filter crew\n",
    "imdb_crew_filtered = imdb_dfs['crew'] \\\n",
    "    .join(imdb_basics_filtered.select(\"tconst\"), \"tconst\", \"inner\")\n",
    "\n",
    "# imdb_crew.write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'crew.parquet'))\n",
    "\n",
    "# For now, leaving `names` alone since it's a people table not directly tied to titleType.\n",
    "imdb_names = imdb_dfs['names']\n",
    "# imdb_dfs['names'].write.mode('overwrite').parquet(os.path.join(PROCESSED_PARQUET_PATH, 'names.parquet'))\n",
    "\n",
    "# Episodes is usually for TV shows, so we can skip it, or handle if needed later.\n",
    "imdb_episodes = imdb_dfs['episode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_basics_filtered.count()\n",
    "imdb_akas_filtered.count()\n",
    "imdb_ratings_filtered.count()\n",
    "imdb_principals_filtered.count()\n",
    "imdb_crew_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_movies.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schemas for IMDb files\n",
    "for name, df in imdb_dfs.items():\n",
    "    print(f\"Schema for IMDb file: {name}\")\n",
    "    df.printSchema()\n",
    "    print(\"=\"*40)  # Just for cleaner separation\n",
    "\n",
    "# Print schema for TMDb file\n",
    "print(\"Schema for TMDb file:\")\n",
    "tmdb_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Highest Rated Movies in 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balancing Ratings and Number of Votes:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter 2023 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Most Popular Actors/Actresses in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. User-Movie Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Metric of a 'Hit Movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#de4a48; background-color:#fce19a; padding: 10px; text-align:left; border: 1px solid #fce19a;\">Predictive Modelling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
